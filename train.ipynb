{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "858ebb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:NumExpr defaulting to 12 threads.\n",
      "c:\\Users\\marci\\miniconda3\\envs\\cv-amm-hw\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "import logging\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "from src.data_loader.data_loader import DataLoader\n",
    "from src.model.model import TwoHeadConvNeXtV2\n",
    "from src.config.configuration import CLASS_NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5227920",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov 22 12:39:45 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.94                 Driver Version: 560.94         CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1050 Ti   WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "|  0%   48C    P8             N/A /   95W |     747MiB /   4096MiB |      7%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A       632    C+G   ...crosoft\\Edge\\Application\\msedge.exe      N/A      |\n",
      "|    0   N/A  N/A      1488    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe      N/A      |\n",
      "|    0   N/A  N/A      2392    C+G   ...les\\Microsoft OneDrive\\OneDrive.exe      N/A      |\n",
      "|    0   N/A  N/A      3552    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A      6020    C+G   ...siveControlPanel\\SystemSettings.exe      N/A      |\n",
      "|    0   N/A  N/A      6040    C+G   ...al\\Discord\\app-1.0.9215\\Discord.exe      N/A      |\n",
      "|    0   N/A  N/A      7296    C+G   ...meCenter\\dlls\\wgc_renderer_host.exe      N/A      |\n",
      "|    0   N/A  N/A      7716    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe      N/A      |\n",
      "|    0   N/A  N/A      8596    C+G   ...64__8wekyb3d8bbwe\\CalculatorApp.exe      N/A      |\n",
      "|    0   N/A  N/A      9444    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A     11716    C+G   ...Programs\\Microsoft VS Code\\Code.exe      N/A      |\n",
      "|    0   N/A  N/A     12700    C+G   ...b3d8bbwe\\Microsoft.Media.Player.exe      N/A      |\n",
      "|    0   N/A  N/A     13960    C+G   ...al\\Discord\\app-1.0.9215\\Discord.exe      N/A      |\n",
      "|    0   N/A  N/A     14148    C+G   ...894_x64__8wekyb3d8bbwe\\ms-teams.exe      N/A      |\n",
      "|    0   N/A  N/A     14176    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A     15036    C+G   ...on\\142.0.3595.90\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     18104    C+G   ...AppData\\Roaming\\Spotify\\Spotify.exe      N/A      |\n",
      "|    0   N/A  N/A     18356    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe      N/A      |\n",
      "|    0   N/A  N/A     18688    C+G   ...on\\142.0.3595.90\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     19100    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A     20724    C+G   ...1.0_x64__8wekyb3d8bbwe\\Video.UI.exe      N/A      |\n",
      "|    0   N/A  N/A     25160    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "logging.info(f\"Using device: {device}\")\n",
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bde3f6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Initializing DataLoader...\n",
      "INFO:root:Checking paths...\n",
      "INFO:root:Loading metadata from data/train_images_metadata.csv...\n",
      "DEBUG:root:Metadata columns: ['observation_id', 'endemic', 'binomial_name', 'code', 'image_path', 'class_id']\n",
      "INFO:root:Loading label info from data/venomous_status_metadata.csv...\n",
      "DEBUG:root:Label info columns: ['class_id', 'MIVS']\n",
      "DEBUG:root:Merged Metadata columns: ['observation_id', 'endemic', 'binomial_name', 'code', 'image_path', 'class_id', 'MIVS']\n",
      "INFO:root:Loading image data from data/train_images_small...\n",
      "Loading metadata:   0%|          | 0/66454 [00:00<?, ?it/s]ERROR:root:Image path data/train_images_small\\1250457.jpg does not exist. Skipping...\n",
      "Loading metadata: 100%|██████████| 66454/66454 [00:21<00:00, 3065.22it/s]\n",
      "INFO:root:Train: 53162, Val: 13291\n"
     ]
    }
   ],
   "source": [
    "img_path = \"data/train_images_small\"\n",
    "label_path = \"data/venomous_status_metadata.csv\"\n",
    "meta_data_path = \"data/train_images_metadata.csv\"\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    image_data_set_path=img_path,\n",
    "    meta_data_path=meta_data_path,\n",
    "    label_info_path=label_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5029976",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using device: cpu\n",
      "INFO:root:Creating TwoHeadConvNeXtV2 with backbone convnextv2_tiny.fcmae\n",
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/convnextv2_tiny.fcmae)\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='huggingface.co' port=443 local_address=None timeout=10 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024BCD555240>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x0000024BCD3E7DC0> server_hostname='huggingface.co' timeout=10\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000024BCD555210>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'HEAD']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'HEAD']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'HEAD']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 302, b'Found', [(b'Content-Type', b'text/plain; charset=utf-8'), (b'Content-Length', b'1333'), (b'Connection', b'keep-alive'), (b'Date', b'Sat, 22 Nov 2025 11:40:09 GMT'), (b'Location', b'https://cas-bridge.xethub.hf.co/xet-bridge-us/63b62e439d50c1463c61e7a7/cce7084275ffaee471651d37ddedaf4fe7139ce711235e4e1c6c5f4d58c61139?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251122%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251122T114009Z&X-Amz-Expires=3600&X-Amz-Signature=ffad1ea01357ea0bc3661f42f4b4a4831a7add1fbf8c0d4edd8604ee55ca4b13&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=6738cf50f31343826f3d9ff1&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&x-id=GetObject&Expires=1763815209&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2MzgxNTIwOX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82M2I2MmU0MzlkNTBjMTQ2M2M2MWU3YTcvY2NlNzA4NDI3NWZmYWVlNDcxNjUxZDM3ZGRlZGFmNGZlNzEzOWNlNzExMjM1ZTRlMWM2YzVmNGQ1OGM2MTEzOSoifV19&Signature=lKBbv-FeDVoHdiOllQPWgOKOnOJLZLe1bDe-g7MBMgQWDdcb5Nmuc05JEzeWOhwmlGTxNNFP28QfZef5meoiyquza2-j4fGo04jGQQf0hJsKhhuXMGUTBOYx9k2c-lfmaSA19s0YE3G8Aq1mS9kuCmvpCd%7E0eblMlxAGYRfkj3dGLCaRMxisiybtYYixuHC6HMTJp1f5tyZCKZ5vSQPQMjbSlPNiq5xRSfvXNE3bR2lnSaeYfdsL03QbpM0Nn6RTcGfN7eTEwUz44TVgNhSWdHly3zDlt38L34G7v8ReVtnlNwzjWq36AFiumzxOIWjkcHIgNacKZf2VX74I4P%7E2uA__&Key-Pair-Id=K2L8F4GPSG1IFC'), (b'X-Powered-By', b'huggingface-moon'), (b'X-Request-Id', b'Root=1-6921a119-449470bc7ca1112e0166948d;d5b20ecd-d3ca-4dd2-b024-723f537e9aee'), (b'RateLimit', b'\"resolvers\";r=4999;t=227'), (b'RateLimit-Policy', b'\"fixed window\";\"resolvers\";q=5000;w=300'), (b'cross-origin-opener-policy', b'same-origin'), (b'Referrer-Policy', b'strict-origin-when-cross-origin'), (b'Access-Control-Max-Age', b'86400'), (b'Access-Control-Allow-Origin', b'https://huggingface.co'), (b'Vary', b'Origin, Accept'), (b'Access-Control-Expose-Headers', b'X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash'), (b'X-Repo-Commit', b'719850d18d9f48ae47b32b8fd0717cd4b82a8b80'), (b'Accept-Ranges', b'bytes'), (b'X-Hub-Cache', b'MISS'), (b'X-Linked-Size', b'111485430'), (b'X-Linked-ETag', b'\"ebf827f53ecf63a942ef912decd42966fef73775ac05666c0229428bbc0f9212\"'), (b'X-Xet-Hash', b'cce7084275ffaee471651d37ddedaf4fe7139ce711235e4e1c6c5f4d58c61139'), (b'Link', b'<https://huggingface.co/api/models/timm/convnextv2_tiny.fcmae/xet-read-token/719850d18d9f48ae47b32b8fd0717cd4b82a8b80>; rel=\"xet-auth\", <https://cas-server.xethub.hf.co/v1/reconstructions/cce7084275ffaee471651d37ddedaf4fe7139ce711235e4e1c6c5f4d58c61139>; rel=\"xet-reconstruction-info\"'), (b'X-Cache', b'Miss from cloudfront'), (b'Via', b'1.1 ff9802edfc33ac761711281d7826a1f8.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'BUD50-P1'), (b'X-Amz-Cf-Id', b'Pc705qUo3DS-dYjkkxZndwmPd_SCj-G-A2LIHUW1hi8j7f0uKqDJhQ==')])\n",
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/timm/convnextv2_tiny.fcmae/resolve/main/model.safetensors \"HTTP/1.1 302 Found\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'HEAD']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "INFO:timm.models._hub:[timm/convnextv2_tiny.fcmae] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n"
     ]
    }
   ],
   "source": [
    "model = TwoHeadConvNeXtV2(num_multi_classes=CLASS_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db2f1f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 96, 56, 56]           4,704\n",
      "       LayerNorm2d-2           [-1, 96, 56, 56]             192\n",
      "          Identity-3           [-1, 96, 56, 56]               0\n",
      "            Conv2d-4           [-1, 96, 56, 56]           4,800\n",
      "         LayerNorm-5           [-1, 56, 56, 96]             192\n",
      "            Linear-6          [-1, 56, 56, 384]          37,248\n",
      "              GELU-7          [-1, 56, 56, 384]               0\n",
      "           Dropout-8          [-1, 56, 56, 384]               0\n",
      "GlobalResponseNorm-9          [-1, 56, 56, 384]             768\n",
      "           Linear-10           [-1, 56, 56, 96]          36,960\n",
      "          Dropout-11           [-1, 56, 56, 96]               0\n",
      "GlobalResponseNormMlp-12           [-1, 56, 56, 96]               0\n",
      "         Identity-13           [-1, 96, 56, 56]               0\n",
      "         Identity-14           [-1, 96, 56, 56]               0\n",
      "    ConvNeXtBlock-15           [-1, 96, 56, 56]               0\n",
      "           Conv2d-16           [-1, 96, 56, 56]           4,800\n",
      "        LayerNorm-17           [-1, 56, 56, 96]             192\n",
      "           Linear-18          [-1, 56, 56, 384]          37,248\n",
      "             GELU-19          [-1, 56, 56, 384]               0\n",
      "          Dropout-20          [-1, 56, 56, 384]               0\n",
      "GlobalResponseNorm-21          [-1, 56, 56, 384]             768\n",
      "           Linear-22           [-1, 56, 56, 96]          36,960\n",
      "          Dropout-23           [-1, 56, 56, 96]               0\n",
      "GlobalResponseNormMlp-24           [-1, 56, 56, 96]               0\n",
      "         DropPath-25           [-1, 96, 56, 56]               0\n",
      "         Identity-26           [-1, 96, 56, 56]               0\n",
      "    ConvNeXtBlock-27           [-1, 96, 56, 56]               0\n",
      "           Conv2d-28           [-1, 96, 56, 56]           4,800\n",
      "        LayerNorm-29           [-1, 56, 56, 96]             192\n",
      "           Linear-30          [-1, 56, 56, 384]          37,248\n",
      "             GELU-31          [-1, 56, 56, 384]               0\n",
      "          Dropout-32          [-1, 56, 56, 384]               0\n",
      "GlobalResponseNorm-33          [-1, 56, 56, 384]             768\n",
      "           Linear-34           [-1, 56, 56, 96]          36,960\n",
      "          Dropout-35           [-1, 56, 56, 96]               0\n",
      "GlobalResponseNormMlp-36           [-1, 56, 56, 96]               0\n",
      "         DropPath-37           [-1, 96, 56, 56]               0\n",
      "         Identity-38           [-1, 96, 56, 56]               0\n",
      "    ConvNeXtBlock-39           [-1, 96, 56, 56]               0\n",
      "    ConvNeXtStage-40           [-1, 96, 56, 56]               0\n",
      "      LayerNorm2d-41           [-1, 96, 56, 56]             192\n",
      "           Conv2d-42          [-1, 192, 28, 28]          73,920\n",
      "           Conv2d-43          [-1, 192, 28, 28]           9,600\n",
      "        LayerNorm-44          [-1, 28, 28, 192]             384\n",
      "           Linear-45          [-1, 28, 28, 768]         148,224\n",
      "             GELU-46          [-1, 28, 28, 768]               0\n",
      "          Dropout-47          [-1, 28, 28, 768]               0\n",
      "GlobalResponseNorm-48          [-1, 28, 28, 768]           1,536\n",
      "           Linear-49          [-1, 28, 28, 192]         147,648\n",
      "          Dropout-50          [-1, 28, 28, 192]               0\n",
      "GlobalResponseNormMlp-51          [-1, 28, 28, 192]               0\n",
      "         DropPath-52          [-1, 192, 28, 28]               0\n",
      "         Identity-53          [-1, 192, 28, 28]               0\n",
      "    ConvNeXtBlock-54          [-1, 192, 28, 28]               0\n",
      "           Conv2d-55          [-1, 192, 28, 28]           9,600\n",
      "        LayerNorm-56          [-1, 28, 28, 192]             384\n",
      "           Linear-57          [-1, 28, 28, 768]         148,224\n",
      "             GELU-58          [-1, 28, 28, 768]               0\n",
      "          Dropout-59          [-1, 28, 28, 768]               0\n",
      "GlobalResponseNorm-60          [-1, 28, 28, 768]           1,536\n",
      "           Linear-61          [-1, 28, 28, 192]         147,648\n",
      "          Dropout-62          [-1, 28, 28, 192]               0\n",
      "GlobalResponseNormMlp-63          [-1, 28, 28, 192]               0\n",
      "         DropPath-64          [-1, 192, 28, 28]               0\n",
      "         Identity-65          [-1, 192, 28, 28]               0\n",
      "    ConvNeXtBlock-66          [-1, 192, 28, 28]               0\n",
      "           Conv2d-67          [-1, 192, 28, 28]           9,600\n",
      "        LayerNorm-68          [-1, 28, 28, 192]             384\n",
      "           Linear-69          [-1, 28, 28, 768]         148,224\n",
      "             GELU-70          [-1, 28, 28, 768]               0\n",
      "          Dropout-71          [-1, 28, 28, 768]               0\n",
      "GlobalResponseNorm-72          [-1, 28, 28, 768]           1,536\n",
      "           Linear-73          [-1, 28, 28, 192]         147,648\n",
      "          Dropout-74          [-1, 28, 28, 192]               0\n",
      "GlobalResponseNormMlp-75          [-1, 28, 28, 192]               0\n",
      "         DropPath-76          [-1, 192, 28, 28]               0\n",
      "         Identity-77          [-1, 192, 28, 28]               0\n",
      "    ConvNeXtBlock-78          [-1, 192, 28, 28]               0\n",
      "    ConvNeXtStage-79          [-1, 192, 28, 28]               0\n",
      "      LayerNorm2d-80          [-1, 192, 28, 28]             384\n",
      "           Conv2d-81          [-1, 384, 14, 14]         295,296\n",
      "           Conv2d-82          [-1, 384, 14, 14]          19,200\n",
      "        LayerNorm-83          [-1, 14, 14, 384]             768\n",
      "           Linear-84         [-1, 14, 14, 1536]         591,360\n",
      "             GELU-85         [-1, 14, 14, 1536]               0\n",
      "          Dropout-86         [-1, 14, 14, 1536]               0\n",
      "GlobalResponseNorm-87         [-1, 14, 14, 1536]           3,072\n",
      "           Linear-88          [-1, 14, 14, 384]         590,208\n",
      "          Dropout-89          [-1, 14, 14, 384]               0\n",
      "GlobalResponseNormMlp-90          [-1, 14, 14, 384]               0\n",
      "         DropPath-91          [-1, 384, 14, 14]               0\n",
      "         Identity-92          [-1, 384, 14, 14]               0\n",
      "    ConvNeXtBlock-93          [-1, 384, 14, 14]               0\n",
      "           Conv2d-94          [-1, 384, 14, 14]          19,200\n",
      "        LayerNorm-95          [-1, 14, 14, 384]             768\n",
      "           Linear-96         [-1, 14, 14, 1536]         591,360\n",
      "             GELU-97         [-1, 14, 14, 1536]               0\n",
      "          Dropout-98         [-1, 14, 14, 1536]               0\n",
      "GlobalResponseNorm-99         [-1, 14, 14, 1536]           3,072\n",
      "          Linear-100          [-1, 14, 14, 384]         590,208\n",
      "         Dropout-101          [-1, 14, 14, 384]               0\n",
      "GlobalResponseNormMlp-102          [-1, 14, 14, 384]               0\n",
      "        DropPath-103          [-1, 384, 14, 14]               0\n",
      "        Identity-104          [-1, 384, 14, 14]               0\n",
      "   ConvNeXtBlock-105          [-1, 384, 14, 14]               0\n",
      "          Conv2d-106          [-1, 384, 14, 14]          19,200\n",
      "       LayerNorm-107          [-1, 14, 14, 384]             768\n",
      "          Linear-108         [-1, 14, 14, 1536]         591,360\n",
      "            GELU-109         [-1, 14, 14, 1536]               0\n",
      "         Dropout-110         [-1, 14, 14, 1536]               0\n",
      "GlobalResponseNorm-111         [-1, 14, 14, 1536]           3,072\n",
      "          Linear-112          [-1, 14, 14, 384]         590,208\n",
      "         Dropout-113          [-1, 14, 14, 384]               0\n",
      "GlobalResponseNormMlp-114          [-1, 14, 14, 384]               0\n",
      "        DropPath-115          [-1, 384, 14, 14]               0\n",
      "        Identity-116          [-1, 384, 14, 14]               0\n",
      "   ConvNeXtBlock-117          [-1, 384, 14, 14]               0\n",
      "          Conv2d-118          [-1, 384, 14, 14]          19,200\n",
      "       LayerNorm-119          [-1, 14, 14, 384]             768\n",
      "          Linear-120         [-1, 14, 14, 1536]         591,360\n",
      "            GELU-121         [-1, 14, 14, 1536]               0\n",
      "         Dropout-122         [-1, 14, 14, 1536]               0\n",
      "GlobalResponseNorm-123         [-1, 14, 14, 1536]           3,072\n",
      "          Linear-124          [-1, 14, 14, 384]         590,208\n",
      "         Dropout-125          [-1, 14, 14, 384]               0\n",
      "GlobalResponseNormMlp-126          [-1, 14, 14, 384]               0\n",
      "        DropPath-127          [-1, 384, 14, 14]               0\n",
      "        Identity-128          [-1, 384, 14, 14]               0\n",
      "   ConvNeXtBlock-129          [-1, 384, 14, 14]               0\n",
      "          Conv2d-130          [-1, 384, 14, 14]          19,200\n",
      "       LayerNorm-131          [-1, 14, 14, 384]             768\n",
      "          Linear-132         [-1, 14, 14, 1536]         591,360\n",
      "            GELU-133         [-1, 14, 14, 1536]               0\n",
      "         Dropout-134         [-1, 14, 14, 1536]               0\n",
      "GlobalResponseNorm-135         [-1, 14, 14, 1536]           3,072\n",
      "          Linear-136          [-1, 14, 14, 384]         590,208\n",
      "         Dropout-137          [-1, 14, 14, 384]               0\n",
      "GlobalResponseNormMlp-138          [-1, 14, 14, 384]               0\n",
      "        DropPath-139          [-1, 384, 14, 14]               0\n",
      "        Identity-140          [-1, 384, 14, 14]               0\n",
      "   ConvNeXtBlock-141          [-1, 384, 14, 14]               0\n",
      "          Conv2d-142          [-1, 384, 14, 14]          19,200\n",
      "       LayerNorm-143          [-1, 14, 14, 384]             768\n",
      "          Linear-144         [-1, 14, 14, 1536]         591,360\n",
      "            GELU-145         [-1, 14, 14, 1536]               0\n",
      "         Dropout-146         [-1, 14, 14, 1536]               0\n",
      "GlobalResponseNorm-147         [-1, 14, 14, 1536]           3,072\n",
      "          Linear-148          [-1, 14, 14, 384]         590,208\n",
      "         Dropout-149          [-1, 14, 14, 384]               0\n",
      "GlobalResponseNormMlp-150          [-1, 14, 14, 384]               0\n",
      "        DropPath-151          [-1, 384, 14, 14]               0\n",
      "        Identity-152          [-1, 384, 14, 14]               0\n",
      "   ConvNeXtBlock-153          [-1, 384, 14, 14]               0\n",
      "          Conv2d-154          [-1, 384, 14, 14]          19,200\n",
      "       LayerNorm-155          [-1, 14, 14, 384]             768\n",
      "          Linear-156         [-1, 14, 14, 1536]         591,360\n",
      "            GELU-157         [-1, 14, 14, 1536]               0\n",
      "         Dropout-158         [-1, 14, 14, 1536]               0\n",
      "GlobalResponseNorm-159         [-1, 14, 14, 1536]           3,072\n",
      "          Linear-160          [-1, 14, 14, 384]         590,208\n",
      "         Dropout-161          [-1, 14, 14, 384]               0\n",
      "GlobalResponseNormMlp-162          [-1, 14, 14, 384]               0\n",
      "        DropPath-163          [-1, 384, 14, 14]               0\n",
      "        Identity-164          [-1, 384, 14, 14]               0\n",
      "   ConvNeXtBlock-165          [-1, 384, 14, 14]               0\n",
      "          Conv2d-166          [-1, 384, 14, 14]          19,200\n",
      "       LayerNorm-167          [-1, 14, 14, 384]             768\n",
      "          Linear-168         [-1, 14, 14, 1536]         591,360\n",
      "            GELU-169         [-1, 14, 14, 1536]               0\n",
      "         Dropout-170         [-1, 14, 14, 1536]               0\n",
      "GlobalResponseNorm-171         [-1, 14, 14, 1536]           3,072\n",
      "          Linear-172          [-1, 14, 14, 384]         590,208\n",
      "         Dropout-173          [-1, 14, 14, 384]               0\n",
      "GlobalResponseNormMlp-174          [-1, 14, 14, 384]               0\n",
      "        DropPath-175          [-1, 384, 14, 14]               0\n",
      "        Identity-176          [-1, 384, 14, 14]               0\n",
      "   ConvNeXtBlock-177          [-1, 384, 14, 14]               0\n",
      "          Conv2d-178          [-1, 384, 14, 14]          19,200\n",
      "       LayerNorm-179          [-1, 14, 14, 384]             768\n",
      "          Linear-180         [-1, 14, 14, 1536]         591,360\n",
      "            GELU-181         [-1, 14, 14, 1536]               0\n",
      "         Dropout-182         [-1, 14, 14, 1536]               0\n",
      "GlobalResponseNorm-183         [-1, 14, 14, 1536]           3,072\n",
      "          Linear-184          [-1, 14, 14, 384]         590,208\n",
      "         Dropout-185          [-1, 14, 14, 384]               0\n",
      "GlobalResponseNormMlp-186          [-1, 14, 14, 384]               0\n",
      "        DropPath-187          [-1, 384, 14, 14]               0\n",
      "        Identity-188          [-1, 384, 14, 14]               0\n",
      "   ConvNeXtBlock-189          [-1, 384, 14, 14]               0\n",
      "   ConvNeXtStage-190          [-1, 384, 14, 14]               0\n",
      "     LayerNorm2d-191          [-1, 384, 14, 14]             768\n",
      "          Conv2d-192            [-1, 768, 7, 7]       1,180,416\n",
      "          Conv2d-193            [-1, 768, 7, 7]          38,400\n",
      "       LayerNorm-194            [-1, 7, 7, 768]           1,536\n",
      "          Linear-195           [-1, 7, 7, 3072]       2,362,368\n",
      "            GELU-196           [-1, 7, 7, 3072]               0\n",
      "         Dropout-197           [-1, 7, 7, 3072]               0\n",
      "GlobalResponseNorm-198           [-1, 7, 7, 3072]           6,144\n",
      "          Linear-199            [-1, 7, 7, 768]       2,360,064\n",
      "         Dropout-200            [-1, 7, 7, 768]               0\n",
      "GlobalResponseNormMlp-201            [-1, 7, 7, 768]               0\n",
      "        DropPath-202            [-1, 768, 7, 7]               0\n",
      "        Identity-203            [-1, 768, 7, 7]               0\n",
      "   ConvNeXtBlock-204            [-1, 768, 7, 7]               0\n",
      "          Conv2d-205            [-1, 768, 7, 7]          38,400\n",
      "       LayerNorm-206            [-1, 7, 7, 768]           1,536\n",
      "          Linear-207           [-1, 7, 7, 3072]       2,362,368\n",
      "            GELU-208           [-1, 7, 7, 3072]               0\n",
      "         Dropout-209           [-1, 7, 7, 3072]               0\n",
      "GlobalResponseNorm-210           [-1, 7, 7, 3072]           6,144\n",
      "          Linear-211            [-1, 7, 7, 768]       2,360,064\n",
      "         Dropout-212            [-1, 7, 7, 768]               0\n",
      "GlobalResponseNormMlp-213            [-1, 7, 7, 768]               0\n",
      "        DropPath-214            [-1, 768, 7, 7]               0\n",
      "        Identity-215            [-1, 768, 7, 7]               0\n",
      "   ConvNeXtBlock-216            [-1, 768, 7, 7]               0\n",
      "          Conv2d-217            [-1, 768, 7, 7]          38,400\n",
      "       LayerNorm-218            [-1, 7, 7, 768]           1,536\n",
      "          Linear-219           [-1, 7, 7, 3072]       2,362,368\n",
      "            GELU-220           [-1, 7, 7, 3072]               0\n",
      "         Dropout-221           [-1, 7, 7, 3072]               0\n",
      "GlobalResponseNorm-222           [-1, 7, 7, 3072]           6,144\n",
      "          Linear-223            [-1, 7, 7, 768]       2,360,064\n",
      "         Dropout-224            [-1, 7, 7, 768]               0\n",
      "GlobalResponseNormMlp-225            [-1, 7, 7, 768]               0\n",
      "        DropPath-226            [-1, 768, 7, 7]               0\n",
      "        Identity-227            [-1, 768, 7, 7]               0\n",
      "   ConvNeXtBlock-228            [-1, 768, 7, 7]               0\n",
      "   ConvNeXtStage-229            [-1, 768, 7, 7]               0\n",
      "        Identity-230            [-1, 768, 7, 7]               0\n",
      "AdaptiveAvgPool2d-231            [-1, 768, 1, 1]               0\n",
      "        Identity-232            [-1, 768, 1, 1]               0\n",
      "SelectAdaptivePool2d-233            [-1, 768, 1, 1]               0\n",
      "     LayerNorm2d-234            [-1, 768, 1, 1]           1,536\n",
      "         Flatten-235                  [-1, 768]               0\n",
      "        Identity-236                  [-1, 768]               0\n",
      "         Dropout-237                  [-1, 768]               0\n",
      "        Identity-238                  [-1, 768]               0\n",
      "NormMlpClassifierHead-239                  [-1, 768]               0\n",
      "        ConvNeXt-240                  [-1, 768]               0\n",
      "          Linear-241                  [-1, 256]         196,864\n",
      "            GELU-242                  [-1, 256]               0\n",
      "         Dropout-243                  [-1, 256]               0\n",
      "          Linear-244                    [-1, 1]             257\n",
      "          Linear-245                  [-1, 512]         393,728\n",
      "            GELU-246                  [-1, 512]               0\n",
      "         Dropout-247                  [-1, 512]               0\n",
      "          Linear-248                  [-1, 296]         151,848\n",
      "================================================================\n",
      "Total params: 28,609,193\n",
      "Trainable params: 28,609,193\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 410.36\n",
      "Params size (MB): 109.14\n",
      "Estimated Total Size (MB): 520.07\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, input_size=(3, 224, 224), device=str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bf5c376",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:1. FÁZIS: Csak a fejek edzése (backbone fagyasztva)\n",
      "Phase1 Epoch 1:   0%|          | 0/1662 [00:00<?, ?it/s]c:\\Users\\marci\\miniconda3\\envs\\cv-amm-hw\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Phase1 Epoch 1:  84%|████████▍ | 1403/1662 [1:58:49<21:56,  5.08s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_model\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\cv-amm-hw\\src\\model\\utils.py:105\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(data_loader, model)\u001b[0m\n\u001b[0;32m    103\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m imgs, ven_lbl, sp_lbl \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPhase1 Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 105\u001b[0m     bin_logit, sp_logit \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m     loss_bin \u001b[38;5;241m=\u001b[39m criterion_bin(bin_logit, ven_lbl)\n\u001b[0;32m    108\u001b[0m     loss_sp  \u001b[38;5;241m=\u001b[39m criterion_sp(sp_logit, sp_lbl)\n",
      "File \u001b[1;32mc:\\Users\\marci\\miniconda3\\envs\\cv-amm-hw\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\marci\\miniconda3\\envs\\cv-amm-hw\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\cv-amm-hw\\src\\model\\model.py:71\u001b[0m, in \u001b[0;36mTwoHeadConvNeXtV2.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m     67\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Visszaad: (binary_logit, multi_logits)\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03m    binary_logit: [B, 1]\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;124;03m    multi_logits: [B, C]\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m     feats: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__backbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;66;03m# backbone may return shape [B, C]\u001b[39;00m\n\u001b[0;32m     73\u001b[0m     b_logit: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__binary_head(feats)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\marci\\miniconda3\\envs\\cv-amm-hw\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\marci\\miniconda3\\envs\\cv-amm-hw\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\marci\\miniconda3\\envs\\cv-amm-hw\\lib\\site-packages\\timm\\models\\convnext.py:626\u001b[0m, in \u001b[0;36mConvNeXt.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    625\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 626\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    627\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_head(x)\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\marci\\miniconda3\\envs\\cv-amm-hw\\lib\\site-packages\\timm\\models\\convnext.py:608\u001b[0m, in \u001b[0;36mConvNeXt.forward_features\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through feature extraction layers.\"\"\"\u001b[39;00m\n\u001b[0;32m    607\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstem(x)\n\u001b[1;32m--> 608\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    609\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_pre(x)\n\u001b[0;32m    610\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\marci\\miniconda3\\envs\\cv-amm-hw\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\marci\\miniconda3\\envs\\cv-amm-hw\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\marci\\miniconda3\\envs\\cv-amm-hw\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;124;03mRuns the forward pass.\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\marci\\miniconda3\\envs\\cv-amm-hw\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\marci\\miniconda3\\envs\\cv-amm-hw\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\marci\\miniconda3\\envs\\cv-amm-hw\\lib\\site-packages\\timm\\models\\convnext.py:306\u001b[0m, in \u001b[0;36mConvNeXtStage.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    304\u001b[0m     x \u001b[38;5;241m=\u001b[39m checkpoint_seq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks, x)\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 306\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\marci\\miniconda3\\envs\\cv-amm-hw\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\marci\\miniconda3\\envs\\cv-amm-hw\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\marci\\miniconda3\\envs\\cv-amm-hw\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;124;03mRuns the forward pass.\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\marci\\miniconda3\\envs\\cv-amm-hw\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\marci\\miniconda3\\envs\\cv-amm-hw\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\marci\\miniconda3\\envs\\cv-amm-hw\\lib\\site-packages\\timm\\models\\convnext.py:208\u001b[0m, in \u001b[0;36mConvNeXtBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    206\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[0;32m    207\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(x)\n\u001b[1;32m--> 208\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mmul(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from src.model.utils import train_model\n",
    "\n",
    "train_model(data_loader, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv-amm-hw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
